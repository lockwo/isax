{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Sparse Ising Machine on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "from isax import (\n",
    "    BlockGraph,\n",
    "    Edge,\n",
    "    IsingModel,\n",
    "    IsingSampler,\n",
    "    Node,\n",
    "    sample_chain,\n",
    "    SamplingArgs,\n",
    ")\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (12665, 784), Test: (2115, 784)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root=\"/tmp/mnist\", train=True, download=True)\n",
    "test_dataset = datasets.MNIST(root=\"/tmp/mnist\", train=False, download=True)\n",
    "\n",
    "train_images = train_dataset.data.numpy()\n",
    "train_labels = train_dataset.targets.numpy()\n",
    "test_images = test_dataset.data.numpy()\n",
    "test_labels = test_dataset.targets.numpy()\n",
    "\n",
    "# Binarize images\n",
    "train_images = (train_images > 127).astype(np.int32)\n",
    "test_images = (test_images > 127).astype(np.int32)\n",
    "\n",
    "# Flatten to vectors\n",
    "train_images = train_images.reshape(train_images.shape[0], -1)\n",
    "test_images = test_images.reshape(test_images.shape[0], -1)\n",
    "\n",
    "target_classes = [0, 1]\n",
    "train_mask = np.isin(train_labels, target_classes)\n",
    "test_mask = np.isin(test_labels, target_classes)\n",
    "\n",
    "train_images = jnp.array(train_images[train_mask])\n",
    "train_labels = jnp.array(train_labels[train_mask])\n",
    "test_images = jnp.array(test_images[test_mask])\n",
    "test_labels = jnp.array(test_labels[test_mask])\n",
    "\n",
    "print(f\"Train: {train_images.shape}, Test: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(train_images[i].reshape(28, 28), cmap=\"gray\")\n",
    "    ax.set_title(f\"Label: {train_labels[i]}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse: 10000 (vs 156800 for RBM)\n",
      "Sparsity: 6.4%\n"
     ]
    }
   ],
   "source": [
    "n_visible = 28 * 28\n",
    "n_hidden = 200\n",
    "n_total = n_visible + n_hidden\n",
    "\n",
    "k_connections = 50  # each hidden unit connects to 50 random visible units\n",
    "\n",
    "key = jax.random.key(42)\n",
    "\n",
    "visible_nodes = [Node() for _ in range(n_visible)]\n",
    "hidden_nodes = [Node() for _ in range(n_hidden)]\n",
    "\n",
    "edges = []\n",
    "key, subkey = jax.random.split(key)\n",
    "for h_idx, h in enumerate(hidden_nodes):\n",
    "    visible_indices = np.random.choice(n_visible, size=(k_connections,), replace=False)\n",
    "    for v_idx in visible_indices:\n",
    "        edges.append(Edge(visible_nodes[int(v_idx)], h))\n",
    "\n",
    "n_edges = len(edges)\n",
    "print(f\"Sparse: {n_edges} (vs {n_visible * n_hidden} for RBM)\")\n",
    "print(f\"Sparsity: {n_edges / (n_visible * n_hidden) * 100:.1f}%\")\n",
    "\n",
    "blocks = [visible_nodes, hidden_nodes]\n",
    "graph = BlockGraph(blocks, edges)\n",
    "params = graph.get_sampling_params()\n",
    "edge_indices, edge_mask = graph.get_edge_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey = jax.random.split(key)\n",
    "init_weights = jax.random.normal(subkey, (n_edges,)) * 0.01\n",
    "\n",
    "data_mean = jnp.mean(train_images, axis=0)\n",
    "eps = 1e-6\n",
    "visible_biases = jnp.log((data_mean + eps) / (1 - data_mean + eps)) * 0.5\n",
    "hidden_biases = jnp.zeros(n_hidden)\n",
    "init_biases = jnp.concatenate([visible_biases, hidden_biases])\n",
    "\n",
    "model = IsingModel(weights=init_weights, biases=init_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_spins(data):\n",
    "    return 2 * data - 1\n",
    "\n",
    "\n",
    "def spins_to_data(spins):\n",
    "    return (spins + 1) // 2\n",
    "\n",
    "\n",
    "sampler = IsingSampler()\n",
    "\n",
    "# Positive phase: only sample hidden (block 1), visible clamped\n",
    "pos_sampling_args = SamplingArgs(\n",
    "    gibbs_steps=1,\n",
    "    blocks_to_sample=[1],\n",
    "    data=params,\n",
    ")\n",
    "\n",
    "# Negative phase: sample both blocks freely\n",
    "neg_sampling_args = SamplingArgs(\n",
    "    gibbs_steps=400,\n",
    "    blocks_to_sample=[0, 1],\n",
    "    data=params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cd_loss(\n",
    "    weights,\n",
    "    biases,\n",
    "    data_batch,\n",
    "    key,\n",
    "):\n",
    "    model = IsingModel(weights=weights, biases=biases)\n",
    "    batch_size = data_batch.shape[0]\n",
    "\n",
    "    visible_spins = data_to_spins(data_batch)\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    hidden_init = jax.random.choice(subkey, jnp.array([-1, 1]), (batch_size, n_hidden))\n",
    "\n",
    "    def sample_positive(visible, hidden, k):\n",
    "        states = sample_chain(\n",
    "            [visible, hidden],\n",
    "            [sampler, sampler],\n",
    "            model,\n",
    "            pos_sampling_args,\n",
    "            jax.random.fold_in(k, 0),\n",
    "        )\n",
    "        return states[0][-1], states[1][-1]\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    keys = jax.random.split(subkey, batch_size)\n",
    "    v_pos, h_pos = jax.vmap(sample_positive)(visible_spins, hidden_init, keys)\n",
    "\n",
    "    def sample_negative(visible, hidden, k):\n",
    "        states = sample_chain(\n",
    "            [visible, hidden],\n",
    "            [sampler, sampler],\n",
    "            model,\n",
    "            neg_sampling_args,\n",
    "            jax.random.fold_in(k, 1),\n",
    "        )\n",
    "        return states[0][-1], states[1][-1]\n",
    "\n",
    "    key, subkey = jax.random.split(key)\n",
    "    keys = jax.random.split(subkey, batch_size)\n",
    "    v_neg, h_neg = jax.vmap(sample_negative)(v_pos, h_pos, keys)\n",
    "\n",
    "    def compute_energy(v, h):\n",
    "        state = jnp.concatenate([v, h])\n",
    "        return model.energy(state, edge_indices, edge_mask)\n",
    "\n",
    "    e_pos = jax.vmap(compute_energy)(v_pos, h_pos)\n",
    "    e_neg = jax.vmap(compute_energy)(v_neg, h_neg)\n",
    "\n",
    "    return jnp.mean(e_pos) - jnp.mean(e_neg)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def cd_step(weights, biases, opt_state, optimizer, data_batch, key):\n",
    "    loss, (grad_w, grad_b) = jax.value_and_grad(cd_loss, argnums=(0, 1))(\n",
    "        weights, biases, data_batch, key\n",
    "    )\n",
    "\n",
    "    updates_w, opt_state_w = optimizer.update(grad_w, opt_state[0], weights)\n",
    "    updates_b, opt_state_b = optimizer.update(grad_b, opt_state[1], biases)\n",
    "\n",
    "    new_weights = optax.apply_updates(weights, updates_w)\n",
    "    new_biases = optax.apply_updates(biases, updates_b)\n",
    "\n",
    "    return new_weights, new_biases, (opt_state_w, opt_state_b), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = optax.adam(learning_rate=learning_rate)\n",
    "opt_state = (optimizer.init(model.weights), optimizer.init(model.biases))\n",
    "\n",
    "weights = model.weights\n",
    "biases = model.biases\n",
    "losses = []\n",
    "\n",
    "n_batches = len(train_images) // batch_size\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    perm = jax.random.permutation(subkey, len(train_images))\n",
    "    train_images_shuffled = train_images[perm]\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx in range(n_batches):\n",
    "        batch = train_images_shuffled[\n",
    "            batch_idx * batch_size : (batch_idx + 1) * batch_size\n",
    "        ]\n",
    "\n",
    "        key, subkey = jax.random.split(key)\n",
    "        weights, biases, opt_state, loss = cd_step(\n",
    "            weights, biases, opt_state, optimizer, batch, subkey\n",
    "        )\n",
    "        epoch_loss += loss\n",
    "\n",
    "    avg_loss = epoch_loss / n_batches\n",
    "    losses.append(float(avg_loss))\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, marker=\"o\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"CD Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_sparse = jnp.zeros((n_visible, n_hidden))\n",
    "\n",
    "edge_to_nodes = []\n",
    "for e in edges:\n",
    "    v_node, h_node = e.nodes\n",
    "    v_idx = visible_nodes.index(v_node)\n",
    "    h_idx = hidden_nodes.index(h_node)\n",
    "    edge_to_nodes.append((v_idx, h_idx))\n",
    "\n",
    "for edge_idx, (v_idx, h_idx) in enumerate(edge_to_nodes):\n",
    "    W_sparse = W_sparse.at[v_idx, h_idx].set(weights[edge_idx])\n",
    "\n",
    "n_show = 16\n",
    "n_cols = 4\n",
    "n_rows = n_show // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < n_hidden:\n",
    "        feature = W_sparse[:, i].reshape(28, 28)\n",
    "        vmax = jnp.max(jnp.abs(feature))\n",
    "        if vmax > 0:\n",
    "            ax.imshow(feature, cmap=\"RdBu_r\", vmin=-vmax, vmax=vmax)\n",
    "        else:\n",
    "            ax.imshow(feature, cmap=\"RdBu_r\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Sparse Hidden Unit Receptive Fields\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_sampling_args = SamplingArgs(\n",
    "    gibbs_steps=1000,\n",
    "    blocks_to_sample=[0, 1],\n",
    "    data=params,\n",
    ")\n",
    "\n",
    "trained_model = IsingModel(weights=weights, biases=biases)\n",
    "sample_fn = eqx.filter_jit(sample_chain)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "@eqx.filter_vmap\n",
    "def generate_sample(key):\n",
    "    k1, k2, k_run = jax.random.split(key, 3)\n",
    "    init_visible = jax.random.choice(k1, jnp.array([-1, 1]), (n_visible,))\n",
    "    init_hidden = jax.random.choice(k2, jnp.array([-1, 1]), (n_hidden,))\n",
    "\n",
    "    samples = sample_fn(\n",
    "        [init_visible, init_hidden],\n",
    "        [sampler, sampler],\n",
    "        trained_model,\n",
    "        gen_sampling_args,\n",
    "        k_run,\n",
    "    )\n",
    "    return samples[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple samples\n",
    "n_gen = 16\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, n_gen)\n",
    "generated_spins = generate_sample(keys)\n",
    "generated = spins_to_data(generated_spins)\n",
    "\n",
    "# Plot\n",
    "n_cols, n_rows = 4, 4\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(generated[i].reshape(28, 28), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Generated Samples (1000 Gibbs steps)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction using isax: clamp visible, sample hidden, then sample visible\n",
    "recon_sampling_args = SamplingArgs(\n",
    "    gibbs_steps=1,\n",
    "    blocks_to_sample=[0, 1],  # One full Gibbs step\n",
    "    data=params,\n",
    ")\n",
    "\n",
    "\n",
    "def reconstruct_sample(visible_data, key):\n",
    "    k1, k_run = jax.random.split(key)\n",
    "    visible_spins = data_to_spins(visible_data)\n",
    "    hidden_init = jax.random.choice(k1, jnp.array([-1, 1]), (n_hidden,))\n",
    "\n",
    "    pos_states = sample_fn(\n",
    "        [visible_spins, hidden_init],\n",
    "        [sampler, sampler],\n",
    "        trained_model,\n",
    "        pos_sampling_args,\n",
    "        k_run,\n",
    "    )\n",
    "\n",
    "    k_run2 = jax.random.fold_in(k_run, 1)\n",
    "    neg_states = sample_fn(\n",
    "        [pos_states[0][-1], pos_states[1][-1]],\n",
    "        [sampler, sampler],\n",
    "        trained_model,\n",
    "        recon_sampling_args,\n",
    "        k_run2,\n",
    "    )\n",
    "\n",
    "    return spins_to_data(neg_states[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = 8\n",
    "test_batch = test_images[:n_test]\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "keys = jax.random.split(subkey, n_test)\n",
    "reconstructed = jax.vmap(reconstruct_sample)(test_batch, keys)\n",
    "\n",
    "fig, axes = plt.subplots(2, n_test, figsize=(16, 4))\n",
    "for i in range(n_test):\n",
    "    axes[0, i].imshow(test_batch[i].reshape(28, 28), cmap=\"gray\")\n",
    "    axes[0, i].axis(\"off\")\n",
    "    if i == 0:\n",
    "        axes[0, i].set_ylabel(\"Original\", fontsize=12)\n",
    "\n",
    "    axes[1, i].imshow(reconstructed[i].reshape(28, 28), cmap=\"gray\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "    if i == 0:\n",
    "        axes[1, i].set_ylabel(\"Reconstructed\", fontsize=12)\n",
    "\n",
    "plt.suptitle(\"Reconstruction Test\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "recon_error = jnp.mean(jnp.abs(test_batch - reconstructed))\n",
    "print(f\"Reconstruction error: {recon_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "@jax.vmap\n",
    "def compute_data_energy(visible_data, key):\n",
    "    visible_spins = data_to_spins(visible_data)\n",
    "    hidden = jax.random.choice(key, jnp.array([-1, 1]), (n_hidden,))\n",
    "\n",
    "    states = sample_fn(\n",
    "        [visible_spins, hidden],\n",
    "        [sampler, sampler],\n",
    "        trained_model,\n",
    "        pos_sampling_args,\n",
    "        key,\n",
    "    )\n",
    "\n",
    "    state = jnp.concatenate([states[0][-1], states[1][-1]])\n",
    "    return trained_model.energy(state, edge_indices, edge_mask)\n",
    "\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "data_keys = jax.random.split(subkey, 100)\n",
    "data_energies = compute_data_energy(train_images[:100], data_keys)\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "noise_keys = jax.random.split(subkey, 100)\n",
    "random_data = jax.random.bernoulli(subkey, 0.5, (100, n_visible)).astype(jnp.int32)\n",
    "noise_energies = compute_data_energy(random_data, noise_keys)\n",
    "\n",
    "print(f\"Data energy: {jnp.mean(data_energies):.2f} +/- {jnp.std(data_energies):.2f}\")\n",
    "print(\n",
    "    f\"Random energy: {jnp.mean(noise_energies):.2f} +/- {jnp.std(noise_energies):.2f}\"\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(data_energies, bins=30, alpha=0.7, label=\"Data\", density=True)\n",
    "plt.hist(noise_energies, bins=30, alpha=0.7, label=\"Random\", density=True)\n",
    "plt.xlabel(\"Energy\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.title(\"Energy Distribution: Data vs Random\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.13 ('jax311')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "37ae68c600525e9075e3fc25a785aab45a51fb3fc6f1a20a7cd54019464c7a03"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
